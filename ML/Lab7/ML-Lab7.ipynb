{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание №7 по курсу «Машинное обучение»: SGD\n",
    "\n",
    "#### Выполнила Мирейко Наталья"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Заданиe\n",
    "\n",
    "\n",
    "Скачайте датасет «Ирисы Фишера» (http://archive.ics.uci.edu/ml/datasets/Iris). В этих данных описаны три вида ирисов. Случайным образом разбейте датасет в соотношении 9 : 1. Первую часть надо будет использовать в качестве обучающей выборки. Вторую — в качестве тестовой (для оценки качества полученной гипотезы). В каждом задании вам необходимо будет построить три модели, по одной модели для каждого вида ирисов. Соответствующая модель должна решать задачу классификации на два класс — принадлежит данный цветок рассматриваемому виду или нет.\n",
    "1. обучите модель логистической регрессии с регуляризацией Тихонова с помощью SGD-алгоритма. Необходимые параметры подберите с помощью алгоритма k-fold.\n",
    "2. обучите модель логистической регрессии с регуляризацией Тихонова с помощью алгоритма batch gradient descent. Этот алгоритм выбирает в качестве направления на каждом шаге −∇LS(w(t)). Необходимые параметры подберите с помощью алгоритм k-fold.\n",
    "3. сравните два полученных решения. Что вы можете сказать о плюсах и минусах каждого из подходов?\n",
    "\n",
    "\n",
    "#### Решениe\n",
    "\n",
    "\n",
    "Был скачан dataset (http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data) и несколько модифицирован сам файл для удобства работы с ним: значение последней колонки с названиями ирисов (Iris-setosa, Iris-versicolor, Iris-virginica) была заменена на номер группы (1, 2, 3) сооответственно.\n",
    "\n",
    "То есть, например, строки dataset'а:\n",
    "\n",
    "\n",
    "5.1,3.5,1.4,0.2,Iris-setosa\n",
    "\n",
    "7.0,3.2,4.7,1.4,Iris-versicolor\n",
    "\n",
    "6.3,3.3,6.0,2.5,Iris-virginica\n",
    "\n",
    "\n",
    "стали выглядеть так:\n",
    "\n",
    "\n",
    "5.1,3.5,1.4,0.2,1\n",
    "\n",
    "7.0,3.2,4.7,1.4,2\n",
    "\n",
    "6.3,3.3,6.0,2.5,3\n",
    "\n",
    "\n",
    "Ниже представлен код Stochastic Gradient Descent и Batch Gradient Descent алгоритмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD - Scores: [0.13251110316402398, 0.13179449369581947, 0.12412050494021755, 0.09453538492297843, 0.1070582295656936, 0.10254315828497079, 0.10828878513564613, 0.11583097203104432, 0.13765248765094681, 0.07347102690135655]\n",
      "SGD - Mean RMSE: 0.113\n",
      "BGD - Scores: [0.13310466200492324, 0.1445656700672241, 0.10681892092464784, 0.11225471032524002, 0.12906259001943918, 0.06842761913904624, 0.11656402320020395, 0.082459246301777, 0.12313445248139471, 0.10281385971159139]\n",
      "BGD - Mean RMSE: 0.112\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression With Stochastic Gradient Descent for Wine Quality\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "from math import sqrt\n",
    " \n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    " \n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    " \n",
    "# Find the min and max values for each column\n",
    "def dataset_minmax(dataset):\n",
    "\tminmax = list()\n",
    "\tfor i in range(len(dataset[0])):\n",
    "\t\tcol_values = [row[i] for row in dataset]\n",
    "\t\tvalue_min = min(col_values)\n",
    "\t\tvalue_max = max(col_values)\n",
    "\t\tminmax.append([value_min, value_max])\n",
    "\treturn minmax\n",
    " \n",
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset, minmax):\n",
    "\tfor row in dataset:\n",
    "\t\tfor i in range(len(row)):\n",
    "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    " \n",
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "\tsum_error = 0.0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tprediction_error = predicted[i] - actual[i]\n",
    "\t\tsum_error += (prediction_error ** 2)\n",
    "\tmean_error = sum_error / float(len(actual))\n",
    "\treturn sqrt(mean_error)\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = linear_regression(algorithm, train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\trmse = rmse_metric(actual, predicted)\n",
    "\t\tscores.append(rmse)\n",
    "\treturn scores\n",
    " \n",
    "# Make a prediction with coefficients\n",
    "def predict(row, coefficients):\n",
    "\tyhat = coefficients[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += coefficients[i + 1] * row[i]\n",
    "\treturn yhat\n",
    " \n",
    "# Estimate linear regression coefficients using stochastic gradient descent\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "\t\t\terror = yhat - row[-1]\n",
    "\t\t\tcoef[0] = coef[0] - l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] - l_rate * error * row[i]\n",
    "\t\t\t# print(l_rate, n_epoch, error)\n",
    "\treturn coef\n",
    "\n",
    "# Estimate linear regression coefficients using batch gradient descent\n",
    "def coefficients_bgd(train, l_rate, n_epoch):\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error_row = [0.0 for i in range(len(train[0]))]\n",
    "\t\tfor row in train:\t\t\t\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "\t\t\terror = yhat - row[-1]\n",
    "\t\t\tcoef[0] = coef[0] - l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tsum_error_row[i] = sum_error_row[i] + error * row[i]\t\n",
    "\t\tfor i in range(len(train[0])-1):\t\n",
    "\t\t\tcoef[i + 1] = coef[i + 1] - l_rate * sum_error_row[i]\n",
    "\t\t\t# print(l_rate, n_epoch, error)\n",
    "\treturn coef\n",
    "\n",
    "# Linear Regression Algorithm With Passed Gradient Descent Algorithm\n",
    "def linear_regression(coefficients_algorithm, train, test, l_rate, n_epoch):\n",
    "\tpredictions = list()\n",
    "\tcoef = coefficients_algorithm(train, l_rate, n_epoch)\n",
    "\tfor row in test:\n",
    "\t\tyhat = predict(row, coef)\n",
    "\t\tpredictions.append(yhat)\n",
    "\treturn(predictions)\n",
    " \n",
    "# Linear Regression on iris dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'iris.data'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# normalize\n",
    "minmax = dataset_minmax(dataset)\n",
    "normalize_dataset(dataset, minmax)\n",
    "# evaluate algorithm\n",
    "n_folds = 10\n",
    "l_rate = 0.01\n",
    "n_epoch = 50\n",
    "sgd_scores = evaluate_algorithm(dataset, coefficients_sgd, n_folds, l_rate, n_epoch)\n",
    "print('SGD - Scores: %s' % sgd_scores)\n",
    "print('SGD - Mean RMSE: %.3f' % (sum(sgd_scores)/float(len(sgd_scores))))\n",
    "bgd_scores = evaluate_algorithm(dataset, coefficients_bgd, n_folds, l_rate, n_epoch)\n",
    "print('BGD - Scores: %s' % bgd_scores)\n",
    "print('BGD - Mean RMSE: %.3f' % (sum(bgd_scores)/float(len(bgd_scores))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге получили незначительную разницу в значении ошибки RMSE двух алгоритмов (SGD и BGD) - примерно 0.001, это объясняется маленьким значением исходной выборки и тем, что эти алгоритмы - вариации gradient descent алгоритма.\n",
    "\n",
    "В алгоритме batch gradient descent на каждом шаге мы проходимся по всем примерам ирисов, чтобы посчитать градиент, и обновляем коэффициенты только в конце каждой итерации-шага epoch. Когда исторических примеров не много, все в порядке, но когда их миллионы, для каждого маленького шага к минимуму мы должны проделать миллионы вычислений, и это может занимать долгое время (то есть трудоемко).\n",
    "\n",
    "Альтернативой этому алгоритму может быть stochastic gradient descent – метод, при котором мы берем какой-то один пример и обновляем значения коэффициентов, ориентируясь только на него. Потом берем следующий пример и обновляем параметры, ориентируясь уже на него. И так далее. Это приводит к тому, что мы не всегда «спускаемся» с холма, иногда мы делаем и шаг вверх или в сторону, но рано или поздно мы достигаем того самого минимума и начинаем кружить вокруг него. Когда значения параметров начинают нас устраивать (достигают нужной нам точности), мы останавливаем спуск. Таким образом, плюсы SGD: не требует хранения всех данных обучения в памяти, а следовательно, хорошо для больших наборов тренировок, позволяет добавлять новые данные в режиме «онлайн».\n",
    "\n",
    "Так же есть особенности схождения алгоритма: batch gradient descent всегда сходится к минимуму при условии, что используется достаточно маленькое значение l_rate. Stochastic gradient descent в общем виде не сходится к минимуму, но есть его модификации, которые позволяют добиться сходимости."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
