{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3 по курсу \"Машинное обучение\": Bias-complexity tradeoff. VC-размерность\n",
    "\n",
    "### Выполнила Мирейко Наталья"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 1.** Вычислите $VCdim(H)$, если $H$ - семейство линейных бинарных классификаторов в $d$-мерном пространстве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение.** \n",
    "\n",
    "Рассмотрим семейство $H$ линейных бинарных классификаторов в  dd -мерном пространстве. Будем полагаем, что классификаторы неоднородные: $$H = \\left\\{h_{a_0, a_1, \\ldots, a_d}(x) = \\begin{cases}1, if a_0+{a_1}{x_1}+\\ldots+{a_d}{x_d}\\ge 0 \\\\0, else \\end{cases} \\right\\}.$$ Докажем, что $VCdim(H) = d+1$, для этого приведем пример множества $C$ с мощностью $\\left|C\\right| = d+1$, являющегося раскрашиваемым. \n",
    "\n",
    "Рассмотрим гипотезу $h_{a_0, a_1, \\ldots, a_d}(x)$, для которой величина $a_0+{a_1}{x_1}+\\ldots+{a_d}{x_d}$ принимает значения $(y_1, \\ldots, y_{d+1})$ на объектах $c_1, \\ldots, c_{d+1}\\in C$, где представим $c_i=(c_{i, 1}, \\ldots, c_{i, d})$, тогда: $$\\begin{cases}a_0+{a_1}{c_{1,1}}+\\ldots+{a_d}{c_{1,d}}=y_1\\\\a_0+{a_1}{c_{2,1}}+\\ldots+{a_d}{c_{2,d}}=y_2\\\\\\ldots\\\\a_0+{a_1}{c_{d+1,1}}+\\ldots+{a_d}{c_{d+1,d}}=y_{d+1}\\end{cases} \\Leftrightarrow \\begin{bmatrix}1&c_{1,1}&\\ldots&c_{1,d}\\\\1&c_{2,1}&\\ldots&c_{2,d}\\\\\\ldots&\\ldots&\\ldots&\\ldots\\\\1&c_{d+1,1}&\\ldots&c_{d+1,d}\\end{bmatrix} \\begin{bmatrix}a_0\\\\a_1\\\\\\ldots\\\\a_d\\end{bmatrix} = \\begin{bmatrix}y_1\\\\y_2\\\\\\ldots\\\\y_{d+1}\\end{bmatrix} \\Leftrightarrow Ca=y.$$ Если определитель матрицы С не равен 0 ($det(C)\\ne 0$), то это матричное уравнение разрешимо относительно $a$: $a=C^{-1}y.$\n",
    "\n",
    "Пусть $с_{i,j} = \\delta_{i-1,j}$, где $\\delta_{i-1,j}$ - символ Кронекера, тогда матрица $C$ имеет вид: $$C = \\begin{bmatrix}1&0&0&0&\\ldots&0\\\\1&1&0&0&\\ldots&0\\\\1&0&1&0&\\ldots&0\\\\1&0&0&1&\\ldots&0\\\\\\ldots&\\ldots&\\ldots&\\ldots&\\ldots&\\ldots\\\\1&0&0&0&\\ldots&1\\end{bmatrix}.$$ Заметим, что $det(C) = 1$, причем: $$a=\\begin{bmatrix}y_1\\\\y_2-y_1\\\\y_3-y_1\\\\\\ldots\\\\y_{d+1}-y_1\\end{bmatrix}.$$\n",
    "\n",
    "Это значит, что для любых значений $y$ (то есть для любой раскраски множества $C$) найдутся коэффициенты $a_0, \\ldots, a_d$, реализующие эту раскраску.\n",
    "\n",
    "Теперь докажем, что если $\\left|C\\right| = d+2$, то разукрасить множество $C$ не получится. Аналогично предыдущим рассуждениям  получим: $$\\begin{bmatrix}1&c_{1,1}&\\ldots&c_{1,d}\\\\1&c_{2,1}&\\ldots&c_{2,d}\\\\\\ldots&\\ldots&\\ldots&\\ldots\\\\1&c_{d+1,1}&\\ldots&c_{d+1,d}\\\\1&c_{d+2,1}&\\ldots&c_{d+2,d}\\end{bmatrix} \\begin{bmatrix}a_0\\\\a_1\\\\\\ldots\\\\a_d\\end{bmatrix} = \\begin{bmatrix}y_1\\\\y_2\\\\\\ldots\\\\y_{d+1}\\\\y_{d+2}\\end{bmatrix}.$$ Эта система является переопределенной ($d+1$ переменная, $d+2$ уравнения). Для существования ее однозначного решения необходимо, чтобы одна из строк матрицы $C$ линейно выражалась через остальные, то есть $\\exists j\\in[d+2]:(1, c_j) = \\sum\\limits_{i=1, i\\ne j}^{d+2}\\alpha_i(1, c_i),$ где $\\sum\\limits_{i=1, i\\ne j}^{d+2}\\alpha_i^2 > 0$. Следовательно $y_j = \\sum\\limits_{i=1, i\\ne j}^{d+2}\\alpha_i y_i$.\n",
    "\n",
    "Так как должны быть достижимы все возможные раскраски, выберем такую, что $y_i \\alpha_i \\ge 0$. Тогда получим, что при таких значениях $y_i$ значение $y_j$ всегда $\\ge 0$, а значит раскраска с $y_j < 0$ недостижима, что и требовалось доказать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3.** Пусть $X$ - булев гиперкуб размерности $n$. Для множества $I\\subseteq\\{1, 2, \\ldots, n\\}$ и объекта $x=(x_1, x_2, \\ldots, x_n)\\in X$ зададим функцию $h_I(x) = \\left(\\sum\\limits_{i\\in I}x_i\\right) \\text{mod } 2$. Чему равна VC-размерность множества всех таких функций?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение.** \n",
    "\n",
    "Введем семейство функций $H_I = \\left\\{h_I(x)\\right\\}$. Тогда $\\left|H_I\\right| = \\sum\\limits_{i=0}^n C_n^i = 2^n \\Rightarrow VCdim(H_I)\\le n$.\n",
    "Докажем, что множество $$C = \\left\\{c_1=(1, 0, \\ldots, 0), c_2=(0, 1, \\ldots, 0), \\ldots, c_n=(0, 0, \\ldots, 1)\\right\\}, \\left|C\\right|=n$$ разукрашиваемо. Обозначим за $h_I(C) = \\left\\{h_I(c_1), \\ldots, h_I(c_n)\\right\\}$. Заметим, что $h_I(c_i)=1 \\Leftrightarrow i\\in I; (1)$. \n",
    "\n",
    "Докажем от противного, что если $I_1 \\ne I_2$, то $h_{I_1}(C) \\ne h_{I_2}(C)$. Пусть $I_1 \\ne I_2$, но $h_{I_1}(C) = h_{I_2}(C)$, то есть $$\\forall i\\in [n]\\space h_{I_1}(c_i) = h_{I_2}(c_i)\\Rightarrow \\left[from\\space (1)\\right] \\Rightarrow\\forall i\\in [n]\\space \\left(i\\in I_1 \\wedge i\\in I_2\\right) \\vee \\left(i \\notin I_1 \\wedge i\\notin I_2\\right); \\space (2)$$, а так как $I_1, I_2 \\subseteq \\{1, 2, \\ldots, n\\}$, то из (2) следует, что $I_1 = I_2$ - противоречие. Таким образом, мы доказали, что $$\\forall I_1, I_2 \\subseteq \\{1, 2, \\ldots, n\\} \\text{ из } I_1 \\ne I_2 \\Rightarrow h_{I_1}(C) \\ne h_{I_2}(C);\\space(3).$$\n",
    "\n",
    "Число пожмножеств множества $\\{1, 2, \\ldots, n\\}$ равно $2^n$. Согласно $(3)$, каждое из этих подмножеств порождает уникальный набор $h_I(C)$, то есть множество $C$ разукрашиваемо, что и требовалось доказать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 4.** Объясните, как согласуются\n",
    "* (ERM-алгоритм над конечным классом $H$ - PAC-learnable в случае гипотезы о реализуемости) и (No Free Lunch Theorem)?\n",
    "* (ERM-алгоритм над конечным классом $H$ - agnostic PAC-learnable) и (No Free Lunch Theorem)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решение.** \n",
    "\n",
    "Согласно *No Free Lunch Theorem*, если у нас есть выборка небольшого размера (меньше, чем половина мощности множества $X$) и ничего не известно о распределении, то оно может быть настолько плохим, что гипотеза, полученная некоторым алгоритмом, с большой вероятностью будет достаточно сильно ошибаться. Согласно \"*ERM-алгоритму над конечным классом $H$ - PAC-learnable в случае гипотезы о реализуемости*\", что если мы знаем все о распределении и размечающей функции, то можем сказать, сколько нам нужно элементов (возможно и больше, чем половина мощности множества $X$), чтобы найти нашим алгоритмом хорошую гипотезу. Эти два утверждения никак не противоречат друг другу. К тому же \"*ERM-алгоритм для конечного класса гипотез $H$ является агностически PAC-изучаемым*\", который использует логику определения PAC-изучаемости (фиксированное распределение и поиск величины выборки), и говорит, что полученная гипотеза будет достаточно хороша относительно лучшей из класса $H$. И это также согласуется с No Free Lunch Theorem, так как в ней оценивается не относительный риск, а абсолютный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
